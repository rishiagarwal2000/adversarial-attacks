{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "second_order_l2_attack.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vapyfO2Q5IUh",
        "outputId": "62af7f66-1d93-4597-cfd1-0794965e0cce"
      },
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.optimizers import SGD\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.compat.v1.disable_v2_behavior()\n",
        "tf.compat.v1.enable_eager_execution(\n",
        "    config=None, device_policy=None, execution_mode=None\n",
        ")\n",
        "import keras"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pR6zYbg5U3v",
        "outputId": "ddf027fe-7682-49eb-d53d-888d9c77225f"
      },
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = mnist.load_data()\n",
        "train_data, test_data = train_data / 255.0, test_data / 255.0\n",
        "\n",
        "# Add a channels dimension\n",
        "train_data = train_data[..., tf.newaxis].astype(\"float32\")\n",
        "test_data = test_data[..., tf.newaxis].astype(\"float32\")\n",
        "\n",
        "\n",
        "VALIDATION_SIZE = 5000\n",
        "validation_data = train_data[:VALIDATION_SIZE, :, :, :]\n",
        "validation_labels = train_labels[:VALIDATION_SIZE]\n",
        "train_data = train_data[VALIDATION_SIZE:, :, :, :]\n",
        "train_labels = train_labels[VALIDATION_SIZE:]\n",
        "params = [32, 32, 64, 64, 200, 200]\n",
        "batch_size = 128\n",
        "\n",
        "# Prepare the training dataset.\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_data, train_labels))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
        "\n",
        "# Prepare the validation dataset.\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((validation_data, validation_labels))\n",
        "val_dataset = val_dataset.batch(batch_size)\n",
        "\n",
        "class MNIST:\n",
        "    def __init__(self, train_data, train_labels, validation_data, validation_labels, test_data, test_labels):\n",
        "        self.train_data = train_data\n",
        "        self.train_labels = train_labels\n",
        "        self.test_data = test_data\n",
        "        self.test_labels = test_labels\n",
        "        self.validation_data = validation_data\n",
        "        self.validation_labels = validation_labels\n",
        "\n",
        "mnist_data = MNIST(train_data, train_labels, validation_data, validation_labels, test_data, test_labels)\n",
        "\n",
        "\n",
        "def show(img):\n",
        "    \"\"\"\n",
        "    Show MNSIT digits in the console.\n",
        "    \"\"\"\n",
        "    remap = \"  .*#\"+\"#\"*100\n",
        "    img = (img.flatten()+.5)*3\n",
        "    if len(img) != 784: return\n",
        "    print(\"START\")\n",
        "    for i in range(28):\n",
        "        print(\"\".join([remap[int(round(x))] for x in img[i*28:i*28+28]]))\n",
        "\n",
        "def get_model(data, file_name, params, num_epochs=50, batch_size=128, train_temp=1, init=None):\n",
        "    \"\"\"\n",
        "    Standard neural network training procedure.\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "\n",
        "    print(data.train_data.shape)\n",
        "    \n",
        "    model.add(Conv2D(params[0], (3, 3),\n",
        "                            input_shape=data.train_data.shape[1:]))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Conv2D(params[1], (3, 3)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(Conv2D(params[2], (3, 3)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Conv2D(params[3], (3, 3)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(params[4]))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(params[5]))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dense(10))\n",
        "    model.add(Activation('softmax'))\n",
        "\n",
        "    return model\n",
        "\n",
        "def get_madry_model(data, file_name, params, num_epochs=50, batch_size=128, train_temp=1, init=None):\n",
        "    \"\"\"\n",
        "    Standard neural network training procedure.\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "\n",
        "    print(data.train_data.shape)\n",
        "    \n",
        "    model.add(Conv2D(32, (5, 5), padding=\"same\",\n",
        "                            input_shape=data.train_data.shape[1:]))\n",
        "    # model.add(Activation('relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), padding=\"same\"))\n",
        "    model.add(Conv2D(64, (5, 5), padding=\"same\"))\n",
        "    # model.add(Activation('relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), padding=\"same\"))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1024))\n",
        "    # model.add(Activation('relu'))\n",
        "    model.add(Dense(10))\n",
        "    model.add(Activation('softmax'))\n",
        "\n",
        "    return model\n",
        "\n",
        "def get_madry_little_diff_model(data, file_name, params, num_epochs=50, batch_size=128, train_temp=1, init=None):\n",
        "    \"\"\"\n",
        "    Standard neural network training procedure.\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "\n",
        "    print(data.train_data.shape)\n",
        "    \n",
        "    model.add(Conv2D(32, (3, 3),\n",
        "                            input_shape=data.train_data.shape[1:]))\n",
        "    # model.add(Activation('relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Conv2D(64, (3, 3)))\n",
        "    # model.add(Activation('relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1024))\n",
        "    # model.add(Activation('relu'))\n",
        "    model.add(Dense(10))\n",
        "    model.add(Activation('softmax'))\n",
        "\n",
        "    return model\n",
        "\n",
        "def get_madry_with_relu_model(data, file_name, params, num_epochs=50, batch_size=128, train_temp=1, init=None):\n",
        "    \"\"\"\n",
        "    Standard neural network training procedure.\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "\n",
        "    print(data.train_data.shape)\n",
        "    \n",
        "    model.add(Conv2D(32, (5, 5), padding=\"same\",\n",
        "                            input_shape=data.train_data.shape[1:]))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), padding=\"same\"))\n",
        "    model.add(Conv2D(64, (5, 5), padding=\"same\"))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), padding=\"same\"))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1024))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dense(10))\n",
        "    model.add(Activation('softmax'))\n",
        "\n",
        "    return model"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJmS-ekK5a_H",
        "outputId": "a2924636-38cb-4c4e-f6e9-f6606d13370f"
      },
      "source": [
        "pgd_model = get_model(mnist_data, \"mnistModel\", params)\n",
        "\n",
        "import keras\n",
        "# Instantiate an optimizer to train the model.\n",
        "optimizer = keras.optimizers.SGD(learning_rate=0.01, decay=1e-6, momentum=0.9)\n",
        "# Instantiate a loss function.\n",
        "loss_fn1 = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "loss_fn2 = keras.losses.BinaryCrossentropy()\n",
        "loss_fn3 = keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.SUM)\n",
        "\n",
        "# Prepare the metrics.\n",
        "train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
        "val_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "\n",
        "# @tf.function\n",
        "def train_step(x1, x2, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        # logits1 = pgd_model(x1, training=True)\n",
        "        logits2 = pgd_model(x2, training=True)\n",
        "        # loss_value = tf.add(loss_fn1(y, logits1), loss_fn1(y, logits2))\n",
        "        loss_value = loss_fn1(y,logits2)\n",
        "    grads = tape.gradient(loss_value, pgd_model.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(grads, pgd_model.trainable_weights))\n",
        "    train_acc_metric.update_state(y, logits2)\n",
        "    return loss_value\n",
        "\n",
        "def normal_train_step(x1,y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits1 = pgd_model(x1, training=True)\n",
        "        # logits2 = pgd_model(x2, training=True)\n",
        "        # loss_value = tf.add(loss_fn1(y, logits1), loss_fn1(y, logits2))\n",
        "        loss_value = loss_fn1(y,logits1)\n",
        "    grads = tape.gradient(loss_value, pgd_model.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(grads, pgd_model.trainable_weights))\n",
        "    train_acc_metric.update_state(y, logits1)\n",
        "    return loss_value\n",
        "\n",
        "# @tf.function\n",
        "def test_step(x, y):\n",
        "    val_logits = pgd_model(x, training=False)\n",
        "    val_acc_metric.update_state(y, val_logits)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(55000, 28, 28, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mk7u8AMd5eTt"
      },
      "source": [
        "\"\"\"\n",
        "Implementation of attack methods. Running this file as a program will\n",
        "apply the attack to the model specified by the config file and store\n",
        "the examples in an .npy file.\n",
        "\"\"\"\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "# import tensorflow as tf\n",
        "# import numpy as np\n",
        "\n",
        "\n",
        "class L2PGDAttack:\n",
        "  def __init__(self, model, epsilon, k, a, random_start, loss_func):\n",
        "    \"\"\"Attack parameter initialization. The attack performs k steps of\n",
        "       size a, while always staying within epsilon from the initial\n",
        "       point.\"\"\"\n",
        "    self.model = pgd_model\n",
        "    self.epsilon = epsilon\n",
        "    self.k = k\n",
        "    self.a = a\n",
        "    self.rand = random_start\n",
        "    self.E_steps = 30\n",
        "    self.sigma = 1000.0/255\n",
        "\n",
        "  def perturb(self, x_nat, y):\n",
        "    \"\"\"Given a set of examples (x_nat, y), returns a set of adversarial\n",
        "       examples within epsilon of x_nat in l_infinity norm.\"\"\"\n",
        "    if self.rand:\n",
        "        x = x_nat + np.random.uniform(-self.epsilon, self.epsilon, x_nat.shape)\n",
        "        x = np.clip(x, 0, 1) # ensure valid pixel range\n",
        "    else:\n",
        "        x = np.copy(x_nat)\n",
        "    # print(x_nat.shape)\n",
        "\n",
        "    for i in range(self.k):\n",
        "        x_tensor = tf.convert_to_tensor(x)\n",
        "        # x_tensor = tf.reshape(x_tensor,)\n",
        "        with tf.GradientTape() as t:\n",
        "            t.watch(x_tensor)\n",
        "            logits1 = pgd_model(x_tensor, training=True)\n",
        "            loss_value = loss_fn3(y, logits1)\n",
        "        result = loss_value\n",
        "        grad = t.gradient(loss_value, x_tensor)\n",
        "        grad = np.zeros(grad.shape)\n",
        "\n",
        "        for j in range(self.E_steps):\n",
        "            d = np.zeros((28*28))\n",
        "            for k in range(28*28):\n",
        "                d[k] = np.random.normal(scale=self.sigma)\n",
        "            d = d.reshape(28,28,1)\n",
        "            x_new = x_tensor + d\n",
        "            with tf.GradientTape() as t:\n",
        "                t.watch(x_new)\n",
        "                logits1 = pgd_model(x_new, training=True)\n",
        "                loss_value = loss_fn3(y, logits1)\n",
        "            result = loss_value\n",
        "            grad1 = t.gradient(loss_value, x_new)\n",
        "            grad = grad + grad1\n",
        "\n",
        "        grad = grad/(self.E_steps)\n",
        "        grad_norm = np.linalg.norm(np.linalg.norm(grad,axis=(1,2)),axis=1).reshape(-1,1,1,1)\n",
        "        grad = grad/grad_norm\n",
        "\n",
        "        x += self.a*grad\n",
        "        delta = tf.reshape((x_nat - x),[-1,28*28])\n",
        "        delta = tf.clip_by_norm(delta, self.epsilon,axes=1)\n",
        "        delta = tf.reshape(delta,[-1,28,28,1])\n",
        "        x = x_nat + delta\n",
        "\n",
        "        x = np.clip(x, 0, 1) # ensure valid pixel range\n",
        "\n",
        "    return x"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Puep88r5gKK"
      },
      "source": [
        "import json\n",
        "import sys\n",
        "import math\n",
        "\n",
        "attack = L2PGDAttack(pgd_model,.3,20,.01,True,\"xent\") "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiskjJtj5jMI",
        "outputId": "4bc49c21-426f-40a7-ce8a-b5bd5fff1c9f"
      },
      "source": [
        "# Training model\n",
        "import time\n",
        "import math \n",
        "std = 0.01\n",
        "epochs = 5\n",
        "pgd_model = get_model(mnist_data, \"mnistModel\", params)\n",
        "\n",
        "total_steps = sum(1 for _ in train_dataset)\n",
        "for epoch in range(epochs):\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Iterate over the batches of the dataset.\n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "        # x_batch_adv = attack.perturb(x_batch_train, y_batch_train).reshape(-1,28,28,1)\n",
        "        # Compute loss\n",
        "        loss_value = normal_train_step(x_batch_train, y_batch_train)\n",
        "        print('\\r', \"Epoch %d\" % (epoch,), 'Training step:', step+1, f'/{total_steps}', 'Loss:', float(loss_value), 'Acc:', float(train_acc_metric.result()), end='')\n",
        "    \n",
        "    # Display metrics at the end of each epoch.\n",
        "    train_acc = train_acc_metric.result()\n",
        "    print(\"\\nTraining acc over epoch: %.4f\" % (float(train_acc),), end=' ')\n",
        "\n",
        "    # Reset training metrics at the end of each epoch\n",
        "    train_acc_metric.reset_states()\n",
        "\n",
        "    # Run a validation loop at the end of each epoch.\n",
        "    for x_batch_val, y_batch_val in val_dataset:\n",
        "        test_step(x_batch_val, y_batch_val)\n",
        "    val_acc = val_acc_metric.result()\n",
        "    val_acc_metric.reset_states()\n",
        "    print(\"Validation acc: %.4f\" % (float(val_acc),), end=' ')\n",
        "    print(\"Time taken: %.2fs\\n\" % (time.time() - start_time))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(55000, 28, 28, 1)\n",
            " Epoch 0 Training step: 430 /430 Loss: 0.1699027270078659 Acc: 0.818363606929779\n",
            "Training acc over epoch: 0.8184 Validation acc: 0.9740 Time taken: 8.03s\n",
            "\n",
            " Epoch 1 Training step: 430 /430 Loss: 0.011361004784703255 Acc: 0.962218165397644\n",
            "Training acc over epoch: 0.9622 Validation acc: 0.9802 Time taken: 8.04s\n",
            "\n",
            " Epoch 2 Training step: 430 /430 Loss: 0.03585405275225639 Acc: 0.9723636507987976\n",
            "Training acc over epoch: 0.9724 Validation acc: 0.9808 Time taken: 7.95s\n",
            "\n",
            " Epoch 3 Training step: 430 /430 Loss: 0.06758473068475723 Acc: 0.9777091145515442\n",
            "Training acc over epoch: 0.9777 Validation acc: 0.9872 Time taken: 8.03s\n",
            "\n",
            " Epoch 4 Training step: 430 /430 Loss: 0.07364551723003387 Acc: 0.9815090894699097\n",
            "Training acc over epoch: 0.9815 Validation acc: 0.9876 Time taken: 7.96s\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1PTav7J5qZw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7861b9d0-5ad8-4552-d87b-0597bd265bd0"
      },
      "source": [
        "import json\n",
        "import sys\n",
        "import math\n",
        "attack = L2PGDAttack(pgd_model, 0.3,20,0.01,True,\"xent\")\n",
        "\n",
        "# Iterate over the samples batch-by-batch\n",
        "num_eval_examples = 10000\n",
        "eval_batch_size = 128\n",
        "num_batches = int(math.ceil(num_eval_examples / eval_batch_size))\n",
        "\n",
        "x_adv = [] # adv accumulator\n",
        "\n",
        "print('Iterating over {} batches'.format(num_batches))\n",
        "\n",
        "for ibatch in range(num_batches):\n",
        "    bstart = ibatch * eval_batch_size\n",
        "    bend = min(bstart + eval_batch_size, num_eval_examples)\n",
        "    print(ibatch)\n",
        "    # print('batch size: {}'.format(bend - bstart))\n",
        "\n",
        "    x_batch = test_data[bstart:bend, :, :, :]\n",
        "    y_batch = test_labels[bstart:bend]\n",
        "\n",
        "    x_batch_adv = attack.perturb(x_batch, y_batch).reshape(-1,28,28,1)\n",
        "    x_adv.append(x_batch_adv)\n",
        "\n",
        "print('Storing examples')\n",
        "path = 'pgd_attack2.npy'\n",
        "x_adv = np.concatenate(x_adv, axis=0)\n",
        "np.save(path, x_adv)\n",
        "print('Examples stored in {}'.format(path))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iterating over 79 batches\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDIOnIOGDTjU"
      },
      "source": [
        "pgd_adv = np.load('pgd_attack2.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a181j6lQph84",
        "outputId": "4d012495-8dd5-4738-d21c-aab89d721261"
      },
      "source": [
        "a = tf.zeros((2,2))\n",
        "print(a.dtype)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<dtype: 'float32'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gezN3anL9UV",
        "outputId": "c5f8c252-0828-47fa-ea12-9c3a077378fe"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygAct764L-8Z"
      },
      "source": [
        "!cp pgd_attack2.npy /content/drive/MyDrive/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLrUplWBMAoo"
      },
      "source": [
        "pgd_adv = np.load('/content/drive/Shareddrives/AML-Project/L2SecondOrder/pgd_attack2.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKS1TFPeF5aV"
      },
      "source": [
        "i = 10\n",
        "x_linf = pgd_adv[i,:,:,:]\n",
        "# show(x_linf)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "im_linf = x_linf.reshape(28, 28)\n",
        "plt.gray()\n",
        "plt.imshow(im_linf)\n",
        "plt.figure()\n",
        "plt.imshow(test_data[i,:,:,:].reshape(28,28))\n",
        "print(test_labels[i])\n",
        "print(np.sum((x_linf-test_data[i,:,:,:])**2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWNmoAy_MCgx"
      },
      "source": [
        "i = 3\n",
        "x_linf = pgd_adv[i,:,:,:]\n",
        "# show(x_linf)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "im_linf = x_linf.reshape(28, 28)\n",
        "plt.gray()\n",
        "plt.imshow(im_linf)\n",
        "plt.figure()\n",
        "plt.imshow(test_data[i,:,:,:].reshape(28,28))\n",
        "print(test_labels[i])\n",
        "print(np.sum((x_linf-test_data[i,:,:,:])**2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-akkdfv4MD4Q"
      },
      "source": [
        "def test_attack_single_input(adv, x, true_label, num_trials=10, std=0.01):\n",
        "    # original input\n",
        "    y = pgd_model(x)\n",
        "    pred_label = np.argmax(y)\n",
        "    count = np.zeros(10)\n",
        "    for i in range(num_trials):\n",
        "        x_noisy = x + np.random.normal(scale=std, size=x.shape)\n",
        "        y_noisy = pgd_model(x_noisy)\n",
        "        noisy_label = np.argmax(y_noisy)\n",
        "        count[noisy_label] += 1\n",
        "    noisy_label = np.argmax(count)\n",
        "\n",
        "    # adv. input\n",
        "    y_adv = pgd_model(adv)\n",
        "    adv_label = np.argmax(y_adv)\n",
        "    count = np.zeros(10)\n",
        "    for i in range(num_trials):\n",
        "        adv_noisy = adv + np.random.normal(scale=std, size=adv.shape)\n",
        "        y_noisy_adv = pgd_model(adv_noisy)\n",
        "        noisy_label_adv = np.argmax(y_noisy_adv)\n",
        "        count[noisy_label_adv] += 1\n",
        "    adv_noisy_label = np.argmax(count)\n",
        "    print('\\r', 'true:', true_label, 'pred:', pred_label, 'noisy_pred:', noisy_label, 'adv_pred:', adv_label, 'adv_noisy_pred:', adv_noisy_label, end='')\n",
        "\n",
        "    return true_label, pred_label, noisy_label, adv_label, adv_noisy_label\n",
        "\n",
        "def test_attack_multiple_inputs(num_runs=100):\n",
        "    corr = 0\n",
        "    noisy_corr = 0\n",
        "    adv_corr = 0\n",
        "    adv_noisy_corr = 0\n",
        "    for run in range(num_runs):\n",
        "        i = np.random.randint(0, 10000)\n",
        "        x = test_data[i,:,:,:][np.newaxis,:,:,:]\n",
        "        adv = pgd_adv[i,:,:,:][np.newaxis,:,:,:]\n",
        "        true, pred, noisy_pred, adv_pred, adv_noisy_pred = test_attack_single_input(adv, x, test_labels[i], num_trials=10, std=0.1)\n",
        "        corr += (pred == true)\n",
        "        noisy_corr += (true == noisy_pred)\n",
        "        adv_corr += (true == adv_pred)\n",
        "        adv_noisy_corr += (true == adv_noisy_pred)\n",
        "    print(f'\\nAcc Original: {corr/num_runs}, Acc Adv: {adv_corr/num_runs}, Acc Adv Noisy: {adv_noisy_corr/num_runs}, Acc Ori Noisy: {noisy_corr/num_runs}')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJhtGb6OMIl9"
      },
      "source": [
        "pgd_model.save_weights('/content/drive/MyDrive/task2a-v2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PU10dyj8MQFw",
        "outputId": "77f5da7f-532a-411c-ebce-0c85971fb42f"
      },
      "source": [
        "pgd_model.load_weights('/content/drive/MyDrive/task2a')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fbf31c4d110>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 690
        },
        "id": "UqQ3dIdqML_x",
        "outputId": "d404278c-0de4-4833-c299-2a633c7d0e10"
      },
      "source": [
        "# Training normal model\n",
        "import time\n",
        "pgd_model = get_madry_with_relu_model(mnist_data, \"mnistModel\", params)\n",
        "epochs = 10\n",
        "total_steps = sum(1 for _ in train_dataset)\n",
        "for epoch in range(epochs):\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Iterate over the batches of the dataset.\n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "        # x_batch_adv = attack.perturb(x_batch_train, y_batch_train).reshape(-1,28,28,1)\n",
        "        # Compute loss\n",
        "        loss_value = normal_train_step(x_batch_train, y_batch_train)\n",
        "        print('\\r', \"Epoch %d\" % (epoch,), 'Training step:', step+1, f'/{total_steps}', 'Loss:', float(loss_value), 'Acc:', float(train_acc_metric.result()), end='')\n",
        "    \n",
        "    # Display metrics at the end of each epoch.\n",
        "    train_acc = train_acc_metric.result()\n",
        "    print(\"\\nTraining acc over epoch: %.4f\" % (float(train_acc),), end=' ')\n",
        "\n",
        "    # Reset training metrics at the end of each epoch\n",
        "    train_acc_metric.reset_states()\n",
        "\n",
        "    # Run a validation loop at the end of each epoch.\n",
        "    for x_batch_val, y_batch_val in val_dataset:\n",
        "        test_step(x_batch_val, y_batch_val)\n",
        "    val_acc = val_acc_metric.result()\n",
        "    val_acc_metric.reset_states()\n",
        "    print(\"Validation acc: %.4f\" % (float(val_acc),), end=' ')\n",
        "    print(\"Time taken: %.2fs\\n\" % (time.time() - start_time))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(55000, 28, 28, 1)\n",
            " Epoch 0 Training step: 430 /430 Loss: 0.17815004289150238 Acc: 0.9060949087142944\n",
            "Training acc over epoch: 0.9061 Validation acc: 0.9756 Time taken: 6.03s\n",
            "\n",
            " Epoch 1 Training step: 430 /430 Loss: 0.02269153483211994 Acc: 0.9777272939682007\n",
            "Training acc over epoch: 0.9777 Validation acc: 0.9764 Time taken: 5.89s\n",
            "\n",
            " Epoch 2 Training step: 430 /430 Loss: 0.007426992058753967 Acc: 0.9846181869506836\n",
            "Training acc over epoch: 0.9846 Validation acc: 0.9864 Time taken: 5.60s\n",
            "\n",
            " Epoch 3 Training step: 430 /430 Loss: 0.01214504148811102 Acc: 0.9882363677024841\n",
            "Training acc over epoch: 0.9882 Validation acc: 0.9862 Time taken: 5.74s\n",
            "\n",
            " Epoch 4 Training step: 430 /430 Loss: 0.005040264688432217 Acc: 0.9907090663909912\n",
            "Training acc over epoch: 0.9907 Validation acc: 0.9896 Time taken: 5.62s\n",
            "\n",
            " Epoch 5 Training step: 150 /430 Loss: 0.06689710170030594 Acc: 0.9921875"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-b680da83ff58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# Compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormal_train_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Epoch %d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Training step:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'/{total_steps}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Loss:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Acc:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_acc_metric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Display metrics at the end of each epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, string)\u001b[0m\n\u001b[1;32m    398\u001b[0m             \u001b[0mis_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_master_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0;31m# only touch the buffer in the IO thread to avoid races\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_child\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m                 \u001b[0;31m# newlines imply flush in subprocesses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mschedule\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_events\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;31m# wake event thread (message content is ignored)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[1;32m    503\u001b[0m                 )\n\u001b[1;32m    504\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSocket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msend_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg_parts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._send_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjMBt8MvMNZr",
        "outputId": "93dcb0fe-a3f9-4b85-ed8a-f38c94cccce0"
      },
      "source": [
        "test_attack_multiple_inputs()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " true: 2 pred: 2 noisy_pred: 2 adv_pred: 2 adv_noisy_pred: 2\n",
            "Acc Original: 1.0, Acc Adv: 1.0, Acc Adv Noisy: 1.0, Acc Ori Noisy: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}