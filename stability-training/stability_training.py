# -*- coding: utf-8 -*-
"""stability-training.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vLeN6SqoGni12COBEue0ZtUBqiFDL69c

## Dependencies
"""

import numpy as np
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Flatten
from keras.layers import Conv2D, MaxPooling2D
from keras.optimizers import SGD

import tensorflow as tf
tf.compat.v1.disable_v2_behavior()
tf.compat.v1.enable_eager_execution(
    config=None, device_policy=None, execution_mode=None
)
import keras

"""## Loading Data"""

mnist = tf.keras.datasets.mnist

(train_data, train_labels), (test_data, test_labels) = mnist.load_data()
train_data, test_data = train_data / 255.0, test_data / 255.0

# Add a channels dimension
train_data = train_data[..., tf.newaxis].astype("float32")
test_data = test_data[..., tf.newaxis].astype("float32")

VALIDATION_SIZE = 5000
validation_data = train_data[:VALIDATION_SIZE, :, :, :]
validation_labels = train_labels[:VALIDATION_SIZE]
train_data = train_data[VALIDATION_SIZE:, :, :, :]
train_labels = train_labels[VALIDATION_SIZE:]

class MNIST:
    def __init__(self, train_data, train_labels, validation_data, validation_labels, test_data, test_labels):
        self.train_data = train_data
        self.train_labels = train_labels
        self.test_data = test_data
        self.test_labels = test_labels
        self.validation_data = validation_data
        self.validation_labels = validation_labels

mnist_data = MNIST(train_data, train_labels, validation_data, validation_labels, test_data, test_labels)

"""Display image"""

## deprecated, code below does not work
# from PIL import Image
# data = train_data[0,:,:,:]
# img = Image.fromarray(data, 'RGB')
# from IPython.display import Image 
# display(img)
# img.save('train.png')

def show(img):
    """
    Show MNSIT digits in the console.
    """
    remap = "  .*#"+"#"*100
    img = (img.flatten()+.5)*3
    if len(img) != 784: return
    print("START")
    for i in range(28):
        print("".join([remap[int(round(x))] for x in img[i*28:i*28+28]]))

show(train_data[1,:,:,:])

"""# Model

### Custom "fit"
* Reference link: https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit
"""

def get_model(data, file_name, params, num_epochs=50, batch_size=128, train_temp=1, init=None):
    """
    Standard neural network training procedure.
    """
    model = Sequential()

    print(data.train_data.shape)
    
    model.add(Conv2D(params[0], (3, 3),
                            input_shape=data.train_data.shape[1:]))
    model.add(Activation('relu'))
    model.add(Conv2D(params[1], (3, 3)))
    model.add(Activation('relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))

    model.add(Conv2D(params[2], (3, 3)))
    model.add(Activation('relu'))
    model.add(Conv2D(params[3], (3, 3)))
    model.add(Activation('relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))

    model.add(Flatten())
    model.add(Dense(params[4]))
    model.add(Activation('relu'))
    model.add(Dropout(0.5))
    model.add(Dense(params[5]))
    model.add(Activation('relu'))
    model.add(Dense(10))
    model.add(Activation('softmax'))

    return model

params = [32, 32, 64, 64, 200, 200]
model = get_model(mnist_data, "mnistModel", params)
batch_size = 128

"""### Preparing dataset compatible with tensorflow"""

# Prepare the training dataset.
train_dataset = tf.data.Dataset.from_tensor_slices((train_data, train_labels))
train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)

# Prepare the validation dataset.
val_dataset = tf.data.Dataset.from_tensor_slices((validation_data, validation_labels))
val_dataset = val_dataset.batch(batch_size)

"""### Optimizer and Loss functions"""

import keras
# Instantiate an optimizer to train the model.
optimizer = keras.optimizers.SGD(learning_rate=0.01, decay=1e-6, momentum=0.9)
# Instantiate a loss function.
loss_fn1 = keras.losses.SparseCategoricalCrossentropy(from_logits=True)
loss_fn2 = keras.losses.BinaryCrossentropy()

# Prepare the metrics.
train_acc_metric = keras.metrics.SparseCategoricalAccuracy()
val_acc_metric = keras.metrics.SparseCategoricalAccuracy()

"""Testing Binary Cross Entropy"""

# Testing binary cross entropy loss
y_true = [[1., 0.], [0.4, 0.6]]
y_pred = [[0.6, 0.4], [0.4, 0.6]]
# Using 'auto'/'sum_over_batch_size' reduction type.
bce = tf.keras.losses.BinaryCrossentropy()
bce(y_true, y_pred)

"""# Writing a training loop from scratch

* Reference link: https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch
"""

# @tf.function
def train_step(x1, x2, y):
    with tf.GradientTape() as tape:
        logits1 = model(x1, training=True)
        logits2 = model(x2, training=True)
        dist = loss_fn2(logits1, logits2)
        if math.isnan(float(dist)):
            print("NAN CASE")
            print("logits1", logits1)
            print(x1)
            print("logits2", logits2)
            print(x2)
            print(model)
        loss_value = tf.add(loss_fn1(y, logits1), dist)
    grads = tape.gradient(loss_value, model.trainable_weights)
    optimizer.apply_gradients(zip(grads, model.trainable_weights))
    train_acc_metric.update_state(y, logits1)
    return loss_value

# @tf.function
def test_step(x, y):
    val_logits = model(x, training=False)
    val_acc_metric.update_state(y, val_logits)

import time
import math 
std = 0.01
epochs = 5

total_steps = sum(1 for _ in train_dataset)
for epoch in range(epochs):
    start_time = time.time()

    # Iterate over the batches of the dataset.
    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):
        # print(np.sum(x_batch_train[0,:,:,:]**2))
        # Generate noisy inputs
        gaussian_noise = np.random.normal(scale=std, size=x_batch_train.shape)
        x_batch_train_added_noise = tf.add(x_batch_train, gaussian_noise)
        
        # Compute loss
        loss_value = train_step(x_batch_train, x_batch_train_added_noise, y_batch_train)
        print('\r', "Epoch %d" % (epoch,), 'Training step:', step+1, f'/{total_steps}', 'Loss:', float(loss_value), 'Acc:', float(train_acc_metric.result()), end='')
    
    # Display metrics at the end of each epoch.
    train_acc = train_acc_metric.result()
    print("\nTraining acc over epoch: %.4f" % (float(train_acc),), end=' ')

    # Reset training metrics at the end of each epoch
    train_acc_metric.reset_states()

    # Run a validation loop at the end of each epoch.
    for x_batch_val, y_batch_val in val_dataset:
        test_step(x_batch_val, y_batch_val)
    val_acc = val_acc_metric.result()
    val_acc_metric.reset_states()
    print("Validation acc: %.4f" % (float(val_acc),), end=' ')
    print("Time taken: %.2fs\n" % (time.time() - start_time))

"""### Test robustness against gaussian noise"""

def test_robustness_single_input(x, std=0.01, num_trials=100):
    y = model(x)
    pred_label = np.argmax(y)
    count = 0
    for i in range(num_trials):
        x_noisy = x + np.random.normal(std, size=x.shape)
        y_noisy = model(x_noisy)
        noisy_label = np.argmax(y_noisy)
        if noisy_label == pred_label:
            count += 1
    print('\r', pred_label, count / num_trials, end='')
    return count / num_trials

def test_robustness_multiple_inputs(std=0.01, num_trials=100):
    score = 0
    for i in range(train_data.shape[0]):
        score += test_robustness_single_input(train_data[i,:,:,:][np.newaxis,:,:,:])
    return score / train_data.shape[0]

test_robustness()